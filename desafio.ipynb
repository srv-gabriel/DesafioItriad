{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600204230528",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "path_train = os.path.join(cwd, 'dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratando valores faltantes\n",
    "\n",
    "O primeiro passo para um bom resultado em um projeto de Aprendizado de Máquina é tratar valores faltantes. Então, a primeira coisa que fazemos aqui é remover os atributos que apresentam quantidade elevada de missing values, que são: 'MiscFeature', 'Fence', 'PoolQC', 'Alley', 'FireplaceQu'. Aplicar imputação nesse atributos geraria um viés muito grande no conjunto de dados devido a números exorbitantes de valores que não são apresentados, como por exemplo 1453 missing values no atributo 'PoolQC', logo a melhor solução é não utilizar tais atributos.\n",
    "\n",
    "Após essa remoção, resta features com pequenas quantidades de valores faltantes, com o maior número sendo 81. Para essas features nós utilizamos técnicas de imputação. Em dados categóricos é utilizada a moda do atributo como valor para substituição, enquanto em dados de valor contínuo a média foi utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo os atributos com muitos dados faltantes\n",
    "df_train = df_train.drop(['MiscFeature', 'Fence', 'PoolQC', 'Alley', 'FireplaceQu'], axis=1)\n",
    "\n",
    "# Aplicando imputação\n",
    "df_train['LotFrontage'] = df_train['LotFrontage'].fillna(df_train['LotFrontage'].mean())\n",
    "df_train['BsmtQual'] = df_train['BsmtQual'].fillna(df_train['BsmtQual'].mode()[0])\n",
    "df_train['BsmtCond'] = df_train['BsmtCond'].fillna(df_train['BsmtCond'].mode()[0])\n",
    "df_train['BsmtExposure'] = df_train['BsmtExposure'].fillna(df_train['BsmtExposure'].mode()[0])\n",
    "df_train['BsmtFinType1'] = df_train['BsmtFinType1'].fillna(df_train['BsmtFinType1'].mode()[0])\n",
    "df_train['BsmtFinType2'] = df_train['BsmtFinType2'].fillna(df_train['BsmtFinType2'].mode()[0])\n",
    "df_train['Electrical'] = df_train['Electrical'].fillna(df_train['Electrical'].mode()[0])\n",
    "df_train['GarageType'] = df_train['GarageType'].fillna(df_train['GarageType'].mode()[0])\n",
    "df_train['GarageYrBlt'] = df_train['GarageYrBlt'].fillna(df_train['GarageYrBlt'].mode()[0])\n",
    "df_train['GarageFinish'] = df_train['GarageFinish'].fillna(df_train['GarageFinish'].mode()[0])\n",
    "df_train['GarageQual'] = df_train['GarageQual'].fillna(df_train['GarageQual'].mode()[0])\n",
    "df_train['GarageCond'] = df_train['GarageCond'].fillna(df_train['GarageCond'].mode()[0])\n",
    "df_train['MasVnrType'] = df_train['MasVnrType'].fillna(df_train['MasVnrType'].mode()[0])\n",
    "df_train['MasVnrArea'] = df_train['MasVnrArea'].fillna(df_train['MasVnrArea'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratando Features nominais\n",
    "\n",
    "Para atributos nominais foi aplicada a numerização, que consiste em substituir as classes de um atributo por números. Esse passo é necessário para que o treino pelos modelos de aprendizado de máquina seja possível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nom_to_num(column):\n",
    "    \"\"\"\n",
    "    Recebe um atributo como parâmetro, transforma cada categoria em números e retorna uma Series do vetor transformado\n",
    "    \"\"\"\n",
    "\n",
    "    # Extrai as diferentes classes do atributo \n",
    "    keys = column.value_counts().keys()\n",
    "    i = 0\n",
    "\n",
    "    # Para cada classe atribui um número\n",
    "    for key in keys:\n",
    "         column = column.replace(to_replace = key, value = i)\n",
    "         i += 1\n",
    "\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nom(df, columns):\n",
    "    \"\"\"\n",
    "    Recebe um DataFrame e os atributos a serem transformados de nominais para numéricos e retorna um DataFrame com esses atributos      transformados\n",
    "    \"\"\"\n",
    "\n",
    "    # Para cada coluna, aplica a função de transformação\n",
    "    for feature in columns:\n",
    "        df[feature] = nom_to_num(df[feature])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas com atributos nominais para serem transformados em númericos\n",
    "\n",
    "columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood', 'Condition1','Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final = handle_nom(df_train, columns).drop('Id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão dos dados\n",
    "\n",
    "Para podermos analisar e melhorar os parâmetros utilizados no regressor, os dados serão separados em conjunto de treino e validação, sendo o conjunto de validação utilizado para fazermos um _tunning_ no modelo. Essa separação é necessária para verificarmos o tradeoff viés-variância antes de aplicarmos o modelo no conjunto de testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_final.drop('SalePrice', axis=1)\n",
    "y = df_train_final['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de modelos\n",
    "\n",
    "    - Os modelos aqui analisados serão: RandomForestRegressor, SVM e uma rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redução de dimensionalidade e Normalização\n",
    "\n",
    "Nesta seção é aplicado o algoritmo de redução de dimensionalidade PCA. Ao utilizarmos algoritmos mais simples, como Random Forest e SVM devemos ter cuidado com a quantidade de features, e.g. dimensão, dos nossos dados, pois algumas características podem inserir ruído e acabar por \"confundir\" o modelo.\n",
    "\n",
    "O PCA é um algoritmo não-supervisionado que analisa a variância entre cada feature e gera uma transformação linear de forma a maximizar essa variância. Com a variância maximizada, características que não acrescentam ao modelo, ou que prejudicam, são retiradas.\n",
    "\n",
    "Para selecionar a quantidade de componentes do PCA que utilizaremos, passamos o valor .95, que indica que queremos o número de features que representam 95% da variância da base de dados. Dessa forma, são retornadas 54 features, que são utilizadas para o Rnadom Forest e para o SVM. Como as features geradas pelo PCA são uma combinação de outras características, elas não apresentam um real significa como a features originais.\n",
    "\n",
    "Para a Rede Neural, devido ao método ser mais robusto e complexo, utilizaremos o conjunto de dados original, com 74 features.\n",
    "\n",
    "Também é aplicada normalização dos dados do conjunto. a normalização é feita de tal forma que a distribuição de todas as características estejam entre 0 e 1. Como o conjunto apresenta uma variação bem grande na distribuição dos dados, a normalização acaba por facilitar a identificação de determinados padrões pelo modelo. A normalização é feita através da subtração da média e divisão pelo desvio padrão em cada feature separadamente\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização dos dados utilizando StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "std.fit(X_train)\n",
    "X_train_std = std.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilização do PCA nos dados normalizados\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_std)\n",
    "sum_var = np.cumsum(pca.explained_variance_ratio_)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plottando a variância explicada em relação à quantidade de componentes dos PCA\n",
    "\n",
    "d = [n for n in range(len(sum_var))]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(d,sum_var, color = 'red',label='cumulative explained variance')\n",
    "plt.ylabel('Cumulative Explained variance')\n",
    "plt.xlabel('Principal components')\n",
    "plt.axhline(y = 95, color='k', linestyle='--', label = '95% Explained Variance')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "\n",
    "O Random Forest Regressor é um algoritmo de ensemble que utiliza o método de Bagging. Funciona da seguinte forma: o conjunto de dados é separado em vários subconjuntos menores, e cada conjunto desse é independente, ou seja, não possuem dados sobressalentes. Então são contruídas várias árvores profundas e treinadas com diferentes subconjuntos de forma que cada árvore extraia diferentes conceitos de cada subconjunto. Então as árvores são agregadas e a média do resultado de todas as árvores é a saída final do Random Forest\n",
    "\n",
    "Em geral, um conjunto de regressores fracos apresentam melhor resultado que um único regressor forte.\n",
    "\n",
    "Foi utilizado o GridSearch Cross-Validation com 10 partições para identificar o conjunto de hiperparâmetros que tem a melhor performance, utilizando RMSE como métrica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Standardization e PCA aos dados\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train_std)\n",
    "X_train_pca = pca.transform(X_train_std)\n",
    "\n",
    "# Construindo um GridSearch para identificar os melhores hiperparâmetros para o regressador\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(),param_grid={ 'max_depth': range(3,7),'n_estimators': (10, 50, 100, 500),},cv=10, scoring='neg_root_mean_squared_error', verbose=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando a busca pelos melhores hiperparâmetros\n",
    "grid_result = grid_search.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultado da busca\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo utilizando os hiperparâmetros encontrados\n",
    "rf_model = RandomForestRegressor(max_depth=grid_result.best_params_['max_depth'], n_estimators=grid_result.best_params_['n_estimators'], random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "rf_model.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resultado do modelo\n",
    "y_pred = rf_model.predict(pca.transform(X_val))\n",
    "rf_result = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print('RMSE = ', round(rf_result, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "O SVM, usualmente muito utilizado para classificação, também pode ser utilizado para regressão. Os princípios do algorítmo continuam o mesmo: definir um hiperplano e maximizar a margem entre os dados. O algorítmo apresenta certa aceitação de erros, o que pode prejudicar sua performance em problemas de regressão, já que a saída é em intervalo contínuo e apresenta infinitas possibilidades. O SVM é um modelo muito robusto e na \"era\" pré Deep Learning era tão usado quanto as redes neurais atualmente, devido a sua capacidade de aprender diversas funções complexas.\n",
    "\n",
    "Fazemos aqui novamente o uso do GridSearch para encontrarmos os melhores parâmetros possíveis dentro de um espaço de busca pré-definido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o espaço de busca para o GridSearch\n",
    "\n",
    "grid_search = GridSearchCV(svm.SVR(), param_grid={'kernel': ('poly', 'rbf', 'sigmoid'), 'degree': (2,3,4,5), 'gamma': np.linspace(0,1, num=5), 'C': (1,5,10,20,80,100, 200)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo a busca pelos melhores hiperparâmetros\n",
    "\n",
    "grid_result = grid_search.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultado da busca\n",
    "\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo utilizando ps hiperparâmetros encontrados\n",
    "\n",
    "svm_model = svm.SVR(kernel=grid_result.best_params_['kernel'], gamma=grid_result.best_params_['gamma'], degree=grid_result.best_params_['degree'], C=grid_result.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "\n",
    "svm_model.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Avaliação do modelo utilizando RMSE\n",
    "\n",
    "y_pred = svm_model.predict(pca.transform(X_val))\n",
    "svm_result = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print('RMSE = ', round(svm_result, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural\n",
    "\n",
    "Um dos maiores passos da computação nesse década, o Deep Learning apresenta robustez maior que os algorítmos de Aprendizagem clássicos apresentados anteriormente. Uma Rede Neural consiste basicamente da tentativa de encontrar o mínimo global do gradiente de uma função de perda. Ela é composta por um número $N$ de camadas e cada camada $N_i$ possui uma quantidade $X_i$ de neurônios e seus respectivos pesos. A rede neural recebe um vetor de features e o resultado baseado na função de ativação que cada camada recebeu. Após isso, é computado o erro utilizando uma função de perda e os pessoas da rede neural são atualizados.\n",
    "\n",
    "Devido a complexidade e robustez do método, foi utilizado o banco de dados original, sem aplicação do PCA ou Normalização das características.\n",
    "\n",
    "**O modelo criado para esse problema utiliza:**\n",
    "\n",
    "    -   4 camadas:\n",
    "        -   primeira camada: 30 neurônios; função de ativação: ReLU; Tamanho de entrada: 74 features;\n",
    "        -   segunda camada: 15 neurônios; função de ativação: ReLU;\n",
    "        -   terceira camada: 20 neurônios; função de ativação: ReLU;\n",
    "        -   camada de saída: 1 neurônios; função de ativação: Linear.\n",
    "    -   função de perda: Mean Squared Error;\n",
    "    -   otimizador: adam;\n",
    "    -   métricas utilizadas: Mean Absolute Error, Mean Squared Logarithmic Error;\n",
    "    -   Épocas: 1000;\n",
    "    -   Batch Size: 10;\n",
    "    -   Conjunto de Validação: 20% do conjunto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os métodos necessários do Keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar o modelo\n",
    "\n",
    "def create_model():\n",
    "    NN = Sequential()\n",
    "    NN.add(Dense(30, input_dim = 74, activation='relu'))\n",
    "    NN.add(Dense(15, activation='relu'))\n",
    "    NN.add(Dense(20, activation='relu'))\n",
    "    NN.add(Dense(1))\n",
    "\n",
    "    return NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "\n",
    "nn_model = create_model()\n",
    "nn_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error', 'mean_squared_logarithmic_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Treinando a rede neural\n",
    "\n",
    "history = nn_model.fit(X, y, epochs=1000, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas para avaliação do modelo\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(np.sqrt(history.history['loss']))\n",
    "plt.plot(np.sqrt(history.history['val_loss']))\n",
    "plt.title('model RMSE')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='best')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='best')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(history.history['mean_squared_logarithmic_error'])\n",
    "plt.plot(history.history['val_mean_squared_logarithmic_error'])\n",
    "plt.title('model MSLE')\n",
    "plt.ylabel('MSLE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_result = np.sqrt(history.history['loss'][-1])\n",
    "print('RMSE Final: ', round(nn_result, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação entre os resultados de cada modelo\n",
    "\n",
    "Como tese para esse Desafio, busco apresentar uma comparação da capacidade de dois modelos clássicos de Aprendizado de Máquina e uma Rede Neural Densa e Superficial.\n",
    "\n",
    "Para a Rede Neural buscou-se o melhor conjunto de hiperparâmetros de forma empírica e foram testados diferentes tratamento dos dados, como os utilizados para o SVM e o Random Forest. A configuração apresentada é a que mostrou melhor resultado em relação ao tradeoff viés-variância. Como foi separado 20% do conjunto para validação, podemos verificar que a diferença do erro de treino e o erro de validação são relativamente pequenos ,levando em consideração o intervalo do vetor 'SalePrice'. O intuito não foi buscar o melhor resultado possível no conjunto de treino, mas sim um equilíbrio entre o erro no conjunto de treino e um erro no conjunto de validação, para que o modelo tenha uma boa performance no conjunto de teste.\n",
    "\n",
    "Já nos métodos clássicos, foi utilizado o GridSearch para fazer uma busca pelo melhores hiperparâmetros. Porém, a rede neural continua com um resultado muito melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('RMSE Random Forest: ', round(rf_result, 4))\n",
    "print('RMSE SVM: ', round(svm_result, 4))\n",
    "print('RMSE Rede Neural: ', round(nn_result, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a raíz quadrada da média do erro ao quadrado (RMSE) podemos verificar a grande diferença entre os resultados obtidos pela rede neural e pelos algorítmos clássicos.\n",
    "\n",
    "Por esse motivo foi decidido utilizar a rede neural construída aqui para fazer a regressão dos dados de teste e geral o resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultado no conjunto de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento dos dados de teste\n",
    "\n",
    "O mesmo processamento aplicado nos dados de treino foi aplicado nos dados de teste, com uma pequena diferença nas features que sofreram imputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = os.path.join(cwd, 'dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['MiscFeature', 'Fence', 'PoolQC', 'Alley', 'FireplaceQu'], axis=1)\n",
    "df_test['LotFrontage'] = df_test['LotFrontage'].fillna(df_test['LotFrontage'].mean())\n",
    "df_test['MSZoning'] = df_test['MSZoning'].fillna(df_test['MSZoning'].mode()[0])\n",
    "df_test['Utilities'] = df_test['Utilities'].fillna(df_test['Utilities'].mode()[0])\n",
    "df_test['Exterior1st'] = df_test['Exterior1st'].fillna(df_test['Exterior1st'].mode()[0])\n",
    "df_test['Exterior2nd'] = df_test['Exterior2nd'].fillna(df_test['Exterior2nd'].mode()[0])\n",
    "df_test['MasVnrType'] = df_test['MasVnrType'].fillna(df_test['MasVnrType'].mode()[0])\n",
    "df_test['MasVnrArea'] = df_test['MasVnrArea'].fillna(df_test['MasVnrArea'].mean())\n",
    "df_test['BsmtQual'] = df_test['BsmtQual'].fillna(df_test['BsmtQual'].mode()[0])\n",
    "df_test['BsmtCond'] = df_test['BsmtCond'].fillna(df_test['BsmtCond'].mode()[0])\n",
    "df_test['BsmtExposure'] = df_test['BsmtExposure'].fillna(df_test['BsmtExposure'].mode()[0])\n",
    "df_test['BsmtFinType1'] = df_test['BsmtFinType1'].fillna(df_test['BsmtFinType1'].mode()[0])\n",
    "df_test['BsmtFinSF1'] = df_test['BsmtFinSF1'].fillna(df_test['BsmtFinSF1'].mean())\n",
    "df_test['BsmtFinType2'] = df_test['BsmtFinType2'].fillna(df_test['BsmtFinType2'].mode()[0])\n",
    "df_test['BsmtFinSF2'] = df_test['BsmtFinSF2'].fillna(df_test['BsmtFinSF2'].mean())\n",
    "df_test['BsmtUnfSF'] = df_test['BsmtUnfSF'].fillna(df_test['BsmtUnfSF'].mean())\n",
    "df_test['TotalBsmtSF'] = df_test['TotalBsmtSF'].fillna(df_test['TotalBsmtSF'].mean())\n",
    "df_test['BsmtFullBath'] = df_test['BsmtFullBath'].fillna(df_test['BsmtFullBath'].mode()[0])\n",
    "df_test['BsmtHalfBath'] = df_test['BsmtHalfBath'].fillna(df_test['BsmtHalfBath'].mode()[0])\n",
    "df_test['KitchenQual'] = df_test['KitchenQual'].fillna(df_test['KitchenQual'].mode()[0])\n",
    "df_test['Functional'] = df_test['Functional'].fillna(df_test['Functional'].mode()[0])\n",
    "df_test['GarageType'] = df_test['GarageType'].fillna(df_test['GarageType'].mode()[0])\n",
    "df_test['GarageYrBlt'] = df_test['GarageYrBlt'].fillna(df_test['GarageYrBlt'].mode()[0])\n",
    "df_test['GarageFinish'] = df_test['GarageFinish'].fillna(df_test['GarageFinish'].mode()[0])\n",
    "df_test['GarageCars'] = df_test['GarageCars'].fillna(df_test['GarageCars'].mode()[0])\n",
    "df_test['GarageArea'] = df_test['GarageArea'].fillna(df_test['GarageArea'].mode()[0])\n",
    "df_test['GarageQual'] = df_test['GarageQual'].fillna(df_test['GarageQual'].mode()[0])\n",
    "df_test['GarageCond'] = df_test['GarageCond'].fillna(df_test['GarageCond'].mode()[0])\n",
    "df_test['SaleType'] = df_test['SaleType'].fillna(df_test['SaleType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = handle_nom(df_test, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação da Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo a coluna Id\n",
    "df_test = df_test.drop('Id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predição para o conjunto de teste\n",
    "test_pred = nn_model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando o vetor resultante em um dataframe\n",
    "df_result = pd.DataFrame(test_pred, columns=['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(cwd, + '/dataset/price_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}